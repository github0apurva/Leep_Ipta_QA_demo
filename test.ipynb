{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Web_page_search_context = \"novartis Iptacopan competitors\"\n",
    "number_of_pages = 10\n",
    "chunk_size = 1000\n",
    "chunk_overlap =20\n",
    "docs_to_vectorize = 2\n",
    "embeddings_to_use ='Ollama'\n",
    "model_to_use = 'Ollama'\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path \n",
    "from box.exceptions import BoxValueError\n",
    "\n",
    "\n",
    "CUSTOM_ENV_PATH = lambda a : '/'.join(os.get_exec_path()[0].split('\\\\')[:-2])+'/'+a\n",
    "CUSTOM_ENV_NAME = CUSTOM_ENV_PATH('chatter.env')\n",
    "\n",
    "def extract_links ( ac, link_in_dict, dict_key ='link' ):\n",
    "    \"\"\"\n",
    "    reads a list of dictionaries and returns specific value for a key \n",
    "    \"\"\"\n",
    "    link_in_list = [ link_in_dict[i][dict_key] for i in range(len(link_in_dict)) ]\n",
    "    print ( \"Activity \", ac, \": Done: extracting webpage from dict\")\n",
    "    return link_in_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novartis Iptacopan competitors for  10  pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study2\\git\\Environments\\novas\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `GoogleSearchAPIWrapper` was deprecated in LangChain 0.0.33 and will be removed in 0.3.0. An updated version of the class exists in the langchain-google-community package and should be used instead. To use it run `pip install -U langchain-google-community` and import as `from langchain_google_community import GoogleSearchAPIWrapper`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity 2: Done: getting webpages from Google wrapper\n",
      "Activity  3 : Done: extracting webpage from dict\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(CUSTOM_ENV_NAME)\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "\n",
    "def extract_related_links ():\n",
    "    \"\"\"\n",
    "    read context and number of results from params.yaml\n",
    "    returns a text file with saved list of web pages to be vectorized\n",
    "    \"\"\"\n",
    "    # extract the context and length of search for websites\n",
    "    search_context =Web_page_search_context\n",
    "    search_num = number_of_pages\n",
    "    print (search_context, \"for \", search_num, \" pages\")\n",
    "\n",
    "    # use the context and length to do a google search using an api wrapper\n",
    "    api_wrapper = GoogleSearchAPIWrapper( siterestrict = False )\n",
    "    link_in_dict = api_wrapper.results (search_context  , search_num )\n",
    "    print ( \"Activity 2: Done: getting webpages from Google wrapper\")\n",
    "    # Extract the links from the above dictionary to a list so as to be used for webbasedloader\n",
    "    link_in_list =  extract_links( 3,  link_in_dict , 'link')\n",
    "\n",
    "    return link_in_list\n",
    "\n",
    "list_link = extract_related_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.novartis.com/news/media-releases/novartis-investigational-iptacopan-phase-iii-study-demonstrates-clinically-meaningful-and-highly-statistically-significant-proteinuria-reduction-patients-iga-nephropathy-igan',\n",
       " 'https://www.biospace.com/article/novartis-pnh-trial-results-put-pressure-on-astrazeneca-s-ultomiris-and-soliris/',\n",
       " 'https://www.novartis.com/research-development/novartis-pipeline',\n",
       " 'https://www.biopharmadive.com/news/novartis-iptacopan-pnh-alexion-complement/634793/',\n",
       " 'https://pharmaphorum.com/news/novartis-oral-drug-iptacopan-tops-injectables-in-head-to-head-pnh-trial',\n",
       " 'https://seekingalpha.com/article/4566810-novartis-competitors-take-note-iptacopan-is-coming',\n",
       " 'https://www.fiercebiotech.com/biotech/novartis-eyeing-3b-market-beats-astrazeneca-blockbuster-phase-3-trigger-race-regulators',\n",
       " 'https://www.clinicaltrialsarena.com/news/novartis-iptacopan-pnh/',\n",
       " 'https://www.fiercebiotech.com/biotech/novartis-rolls-out-encouraging-phase-2-data-rare-kidney-diseases-little-competition-sight',\n",
       " 'https://www.clinicaltrialsarena.com/news/novartis-eyes-accelerated-fda-approval-for-iptacopan-across-multiple-indications/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study2\\git\\Environments\\novas\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path \n",
    "import os\n",
    "\n",
    "from src.constants import *\n",
    "from src.utils import read_yaml, read_from_text\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(CUSTOM_ENV_NAME)\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "import streamlit as st\n",
    "import time\n",
    "\n",
    "def create_retreiver (links_to_load):\n",
    "    \"\"\"\n",
    "    read chunk size and overlap from params.yaml\n",
    "    \"\"\"\n",
    "    # # read the list of webpage to be taken\n",
    "    # links_to_load = read_from_text ( 5, WEB_PAGE_TXT )\n",
    "\n",
    "    # use a loader to get all content\n",
    "    loader = WebBaseLoader(web_paths = links_to_load,)\n",
    "                    #        bs_kwargs= dict(parse_only = bs4.SoupStrainer(\n",
    "                    #        class_ = (\"post-title\",\"post-content\",\"post-header\")\n",
    "                    #    )),)    \n",
    "    web_docs = loader.load()\n",
    "    print (\"size of the web docs : \",len(web_docs))\n",
    "    print (\"Activity \", 6, \": Done: webpage content from links\")\n",
    "    \n",
    "    # extract the size and overlap chars from param file\n",
    "    print (chunk_size, \"is the chunk size choosen with \", chunk_overlap, \" chars overlaps \")\n",
    "\n",
    "    # splitting the docs based on above size etc\n",
    "    documents = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap).split_documents(web_docs)\n",
    "    # taking only top n docs\n",
    "    documents_filtered = documents[:2]\n",
    "    # choosing the embeddings to use\n",
    "    if embeddings_to_use == \"Instruct\":\n",
    "        print ( \"Instruct Embeddings\" )\n",
    "        choosen_embeddings = HuggingFaceInstructEmbeddings(model_name = 'hkunlp/instructor-xl',  \n",
    "                                                   model_kwargs={\"device\":\"cpu\"},\n",
    "                                                   encode_kwargs ={'normalize_embeddings': True} )\n",
    "    else :\n",
    "        choosen_embeddings = OllamaEmbeddings()\n",
    "        print ( \"Ollama Embeddings\" )\n",
    "    # creatign vector store and eventually retreiver\n",
    "    verctordb = FAISS.from_documents(documents_filtered, choosen_embeddings )\n",
    "    #retriever = verctordb.as_retriever()\n",
    "    return verctordb\n",
    "\n",
    "def get_retrieval_chain (retriever):\n",
    "    # get prompt\n",
    "    # prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "    # print( prompt.messages )\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the questions based on the provided context only.\n",
    "    Please provide the most accurate response based on the question\n",
    "    <context>\n",
    "    {context}\n",
    "    <context>\n",
    "    Questions: {input}\n",
    "    \"\"\")\n",
    "\n",
    "    #llm = Ollama(model = \"llama2\")\n",
    "    llm= ChatOllama(model=\"llama2\")\n",
    "\n",
    "    # creating document chain\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    retrieval_chain = create_retrieval_chain (retriever, document_chain)\n",
    "    return retrieval_chain\n",
    "\n",
    "def talk_to_stream (retrieval_chain):\n",
    "    st.title(\"Demo with LLAMA2\")\n",
    "\n",
    "    input_text = st.text_input (\"Input the question you have about Iptacopan \")\n",
    "    #input_text = \"what is Iptacopan\"\n",
    "    if input_text:\n",
    "        start_time = time.process_time()\n",
    "        response = retrieval_chain.invoke ({\"input\":input_text})\n",
    "        print (\"response time: \",time.process_time()-start_time)\n",
    "        print ( response['answer'] )\n",
    "        st.write(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the web docs :  10\n",
      "Activity  6 : Done: webpage content from links\n",
      "1000 is the chunk size choosen with  20  chars overlaps \n",
      "Ollama Embeddings\n"
     ]
    }
   ],
   "source": [
    "# retr = create_retreiver (list_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the web docs :  10\n",
      "Activity  6 : Done: webpage content from links\n",
      "1000 is the chunk size choosen with  20  chars overlaps \n",
      "Ollama Embeddings\n"
     ]
    }
   ],
   "source": [
    "vcdb = create_retreiver (list_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcdb.save_local ( os.getcwd() , 'nova_vector_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Study2\\\\git\\\\Leep_Ipta_QA_demo'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcdb_load = FAISS.load_local ( 'd:/Study2/git/Leep_Ipta_QA_demo',index_name  = 'nova_vector_db',\n",
    "                               embeddings = OllamaEmbeddings() ,allow_dangerous_deserialization = True   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc  = get_retrieval_chain (retr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 23:30:50.296 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run d:\\Study2\\git\\Environments\\novas\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response time:  0.328125\n",
      "Iptacopan is an investigational drug developed by Novartis for the treatment of IgA nephropathy (IgAN), a kidney disease. According to the context provided, the Phase III study of iptacopan has demonstrated clinically meaningful and highly statistically significant proteinuria reduction in patients with IgAN.\n"
     ]
    }
   ],
   "source": [
    "talk_to_stream ( rc )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
